# 用户/内核空间

为了保证操作系统的安全性，一个进程的地址空间分为 **用户空间** 和 **内核空间**。

我们平常运行的应用程序都是运行在用户空间，只有内核空间才可以进行系统态级别的资源相关操作，比如文件管理、进程通信、内存管理等。

并且用户空间不能直接访问内核空间，如用户进程要执行 IO 操作，必须通过**系统调用**来间接请求操作系统。

应用程序发起 IO 调用时，会经历两个步骤：

1. 内核等待 IO 设备准备好数据。
2. 内核将数据从内核空间拷贝到用户空间。

# IO调用和IO执行

- IO调用: 进程向内核发起系统调用
- IO执行:(系统调用 recvfrom) 
  - 准备阶段: 内核等待IO设备准备好数据
  - 拷贝阶段: 将数据从内核缓冲区拷贝到用户缓冲区(线程缓冲区)中

# 阻塞/非阻塞-异步/同步

- 阻塞/非阻塞，即线程在发起调用后，是否会**阻塞等待或者轮询** IO 执行完成，如果会等待，则线程变为阻塞状态，即阻塞IO；若不等待，即继续运行状态，即非阻塞IO。

  **即阻塞非阻塞关注的是线程调用的函数的行为，即调用的函数是否会阻塞该线程。**

- 异步/同步，即线程在发起调用后，是否会继续执行，如果会继续执行，则为异步IO，如果会等待调用完成后继续执行，则为同步IO。

  **即同步异步关注的是线程自身的行为，是否会等待函数的结果。**

# 阻塞IO(BIO)

在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当IO操作完成之后，用户进程才能运行。

> BIO 中，应用程序发起 read 调用后，会一直阻塞，直到内核将数据拷贝到用户空间。

![图片](..\img\BIO2)

# 非阻塞IO(NIO)

在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。JAVA的NIO就属于同步非阻塞IO。

![图片](..\img\NIO2)

# IO 多路复用

多路复用中，通过select函数，可以同时监听多个IO请求的内核操作，只要有任意一个IO的内核操作就绪，都可以通知select函数返回，再进行系统调用recvfrom()完成IO操作。

这个过程应用程序就可以同时监听多个IO请求，这比起基于多线程阻塞式IO要先进得多，因为服务器只需要少数线程就可以进行大量的客户端通信。

![图片](..\img\多路复用2)

## 机制

该机制是通过 **多路复用器** 实现的。

多路复用器可以让单个线程监视多个客户端(socket)、文件，当客户端(socket)、文件数据准备就绪后，通知目标线程发起 read 调用。

> 文件描述符 (File descriptor) 用于表述指向文件的引用的抽象化概念。

- select 机制

  select 机制中提供一个 **fd_set** 的数据结构，实际是 Long 类型的数组，存储的是文件描述符。当**调用 select()** 时，**内核**根据 IO 状态来修改 fd_set 的内容，由此来通知执行了 select() 的进程哪个 **socket 或 文件** 可读。

  问题：

  - 每次调用 select() 都需要将 fd_set 从**用户态拷贝到内核态**。
  - 同时遍历 fd_set
  - 为了减少拷贝的性能损坏，内核对 fd_set 大小做了限制。

- poll 机制

  poll 机制与 select 类似，只是解决了大小限制问题。

- epoll 机制

  epoll 机制是基于**事件驱动**的IO方式，**解决了大小限制**。

  epoll 提供的函数

  ```c
  int epoll_create(int size);
  int epoll_ctl(...);
  int epoll_wait(...);
  ```

  - epoll_create 创建**epoll 句柄**，函数声明要监听的文件描述符数量
  - epoll_ctl 函数在**内核**中**注册**要监听的文件描述符，只需一次用户态到内核态拷贝，可以**解决 select/poll 每次调用都需要将 fd_set 从用户态拷贝到内核态的问题**。
  - epoll_wait() 函数等待事件的就绪，即**事件驱动**，可以**解决需要遍历 fd_set 的问题**。

![img](..\img\0f483f2437ce4ecdb180134270a00144~tplv-k3u1fbpfcp-watermark.image)

## 两种模型

- Level Triggered 即 **TL 水平触发**

  .socket接收缓冲区不为空 有数据可读 读事件一直触发
  .socket发送缓冲区不满 可以继续写入数据 写事件一直触发
  符合思维习惯，epoll_wait返回的事件就是socket的状态

- Edge Triggered 即 **ET 边沿触发** 

  .socket的接收缓冲区状态变化时触发读事件，即空的接收缓冲区刚接收到数据，或者有新数据到达时，触发读事件。
  .socket的发送缓冲区状态变化时触发写事件，即满的缓冲区刚空出空间时（由满状态变为不满状态时），或者待写数据变少时，触发读事件
  仅在状态变化时触发事件

### LT

LT的处理过程：
. accept一个连接，添加到epoll中监听EPOLLIN事件
. 当EPOLLIN事件到达时，read fd中的数据并处理
. 当需要写出数据时，把数据write到fd中；如果数据较大，无法一次性写出，那么在epoll中监听EPOLLOUT事件
. 当EPOLLOUT事件到达时，继续把数据write到fd中；如果数据写出完毕，那么在epoll中关闭EPOLLOUT事件

#### 读事件

因为LT的事件可以一直触发，所以对读事件的处理可以根据自己的需求，自己决定是一次性读取完，还是分段的读。如果接收缓冲区满了，那么其就不能接收数据了，**但是读事件还是会一直触发（因为你没有去接收数据）**。解决方法如下：

- 修改fd的注册事件类型
- 或者把fd移出事件表

#### 写事件

因为LT的事件可以一直触发，所以对写事件的处理可以根据自己的需求，自己决定是一次性写完，还是分段的写。如果发送缓冲区满了，那么其就不能发送数据了，**但是写事件还是会一直触发（因为你没有把数据写出去）。**解决方法如下：

- 修改fd的注册事件类型
- 或者把fd移出事件表

### ET

. accept一个一个连接，添加到epoll中监听EPOLLIN|EPOLLOUT事件
. 当EPOLLIN事件到达时，read fd中的数据并处理，read需要一直读，直到返回EAGAIN为止
. 当需要写出数据时，把数据write到fd中，直到数据全部写完，或者write返回EAGAIN
. 当EPOLLOUT事件到达时，继续把数据write到fd中，直到数据全部写完，或者write返回EAGAIN

从ET的处理过程中可以看到，ET的要求是需要一直读写，直到返回EAGAIN，否则就会遗漏事件。而LT的处理过程中，直到返回EAGAIN不是硬性要求，但通常的处理过程都会读写直到返回EAGAIN，但LT比ET多了一个开关EPOLLOUT事件的步骤

#### **读事件**

因为ET的事件只能触发一次，所以如果不想要接收的数据丢失，那么就需要在***\*这次事件触发的时候一次性把数据读完\****（一般是读到EAGAIN）。如果读取的时候，接收缓冲区满了，那就需要应用层自行标记，解决OS不再通知可读的问题

#### **写事件**

因为ET的事件只能触发一次，所以如果想要不想让发送的数据没有全部发送出去，那么就需要在***\*这次事件触发的时候一次性把数据写完\****（一般是写到EAGAIN）。如果发送的时候，发送缓冲区满了，那就需要应用层自行标记，解决OS不再通知可写的问题

#### accept事件

- **ET模式下的accept()应该是非阻塞的：**
  - 如果套接口被设置成阻 塞模式，服务器就会一直阻塞在 accept 调用上，直到其他某个客户建立一个新的连接 为止。但是在此期间，服务器单纯地阻塞在 accept 调用上，就绪队列中的其他描述符 都得不到处理
  - 解决办法是把监听套接口设置为非阻塞，当客户在服务器调用 accept 之前中止某个连 接时，accept 调用可以立即返回-1，这时源自 Berkeley 的实现会在内核中处理该事件，并不会将该事件通知给epoll，而其他实现把 errno 设置为 ECONNABORTED 或者 EPROTO 错误，我们应该忽略这两个错误
- **ET模式下使用while处理accept()：**
  - 考虑这种情况：多个连接同时到达，服务器的 TCP 就绪队列瞬间积累多个就绪连接，由 于是边缘触发模式，epoll 只会通知一次，accept 只处理一个连接，导致 TCP 就绪队列 中剩下的连接都得不到处理。
  - 解决办法是用 while 循环抱住 accept 调用，处理完 TCP 就绪队列中的所有连接后再退出循环。如何知道是否处理完就绪队列中的所有连接呢？accept 返回-1 并且 errno 设置为 EAGAIN 就表示所有连接都处理完
- **综上所述：**服务器应该使用非阻塞的accept，accept 在ET模式下的正确使用方式如下所示：

### **应用场景**

- **大数据处理：**因为大数据的数据量比较多，因此一次可能处理不完，可以使用水平触发，来多次处理数据
- **小数据处理：**小数据调用边缘触发即可，一次处理完就行
- **服务器的监听套接字：**使用水平触发。当有客户端连接时如果这次不处理，可以放到下一次来处理。但是如果使用边缘触发，本次不处理，下次再处理就消失了，从而失去了这个客户端的连接

# 信号驱动IO

在unix系统中，应用程序发起IO请求时，可以给IO请求注册一个信号函数，请求立即返回，操作系统底层则处于等待状态（等待数据就绪），直到数据就绪，然后通过信号通知主调程序，主调程序才去调用系统函数recvfrom()完成IO操作。

信号驱动也是一种非阻塞式的IO模型，比起上面的非阻塞式IO模型，**信号驱动式IO模型不需要轮询检查底层IO数据是否就绪**，而是被动接收信号，然后再调用recvfrom执行IO操作。

比起多路复用IO模型来说，信号驱动IO模型**针对的是一个IO的完成过程**， 而多路复用IO模型针对的是多个IO同时进行时候的场景。 信号驱动式IO模型用下图表示

![图片](..\img\信号驱动IO)

# 异步IO(AIO)

在此种模式下，将整个IO操作（包括等待数据就绪，复制数据到应用程序工作空间）全都交给操作系统完成。数据就绪后操作系统将数据拷贝进应用程序运行空间之后，操作系统再通知应用程序，这个过程中应用程序不需要阻塞。

![图片](..\img\异步IO)

# 误区

**阻塞、非阻塞、多路IO复用，都是同步IO，异步必定是非阻塞的**，所以不存在异步阻塞和异步非阻塞的说法。真正的异步IO需要CPU的深度参与。换句话说，只有用户线程在操作IO的时候根本不去考虑IO的执行，全部都交给CPU去完成，而只需要等待一个完成信号的时候，才是真正的异步IO。所以，fork子线程去轮询、死循环或者使用select、poll、epoll，都不是异步。

# 适用场景

- BIO方式适用于**连接数目比较小且固定的架构**，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。
- NIO方式适用于**连接数目多且连接比较短（轻操作）的架构**，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。
- AIO方式适用于**连接数目多且连接比较长（重操作）的架构**，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。

https://mp.weixin.qq.com/s?__biz=MzUyNzgyNzAwNg==&mid=2247483941&idx=1&sn=97628f4d69d8607badf39bfeb7557457&scene=21#wechat_redirect

