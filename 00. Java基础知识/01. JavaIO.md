# 用户/内核空间

为了保证操作系统的安全性，一个进程的地址空间分为 **用户空间** 和 **内核空间**。

我们平常运行的应用程序都是运行在用户空间，只有内核空间才可以进行系统态级别的资源相关操作，比如文件管理、进程通信、内存管理等。

并且用户空间不能直接访问内核空间，如用户进程要执行 IO 操作，必须通过**系统调用**来间接请求操作系统。

应用程序发起 IO 调用时，会经历两个步骤：

1. 内核等待 IO 设备准备好数据。
2. 内核将数据从内核空间拷贝到用户空间。

# IO调用和IO执行

- IO调用: 进程向内核发起系统调用
- IO执行:(系统调用 recvfrom) 
  - 准备阶段: 内核等待IO设备准备好数据
  - 拷贝阶段: 将数据从内核缓冲区拷贝到用户缓冲区(线程缓冲区)中

# 阻塞/非阻塞-异步/同步

- 阻塞/非阻塞，即线程在发起调用后，是否会**阻塞等待或者轮询** IO 执行完成，如果会等待，则线程变为阻塞状态，即阻塞IO；若不等待，即继续运行状态，即非阻塞IO。

  **即阻塞非阻塞关注的是线程调用的函数的行为，即调用的函数是否会阻塞该线程。**

- 异步/同步，即线程在发起调用后，是否会继续执行，如果会继续执行，则为异步IO，如果会等待调用完成后继续执行，则为同步IO。

  **即同步异步关注的是线程自身的行为，是否会等待函数的结果。**

# 阻塞IO(BIO)

在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当IO操作完成之后，用户进程才能运行。

> BIO 中，应用程序发起 read 调用后，会一直阻塞，直到内核将数据拷贝到用户空间。

![图片](/Users/bytedance/GithubProject/basic-knowledge/03. 操作系统/..\img\BIO2)

# 非阻塞IO(NIO)

在此种方式下，用户进程发起一个IO操作以后可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。JAVA的NIO就属于同步非阻塞IO。

![图片](/Users/bytedance/GithubProject/basic-knowledge/03. 操作系统/..\img\NIO2)

# IO 多路复用

多路复用中，通过**select函数**，可以同时监听多个IO请求的内核操作，只要有任意一个IO的内核操作就绪，都可以通知select函数返回，再进行系统调用recvfrom()完成IO操作。

这个过程应用程序就可以同时监听多个IO请求，这比起基于多线程阻塞式IO要先进得多，因为服**务器只需要少数线程就可以进行大量的客户端通信。**

![图片](/Users/bytedance/GithubProject/basic-knowledge/03. 操作系统/..\img\多路复用2)

## 机制

该机制是通过 **多路复用器** 实现的。

多路复用器可以让**单个线程**监视多个客户端(socket)、文件，当客户端(socket)、文件数据准备就绪后，通知目标线程发起 read 调用。

> 文件描述符 (File descriptor) 用于表述指向文件的引用的抽象化概念。

- select 机制

  select 机制中提供一个 **fd_set** 的数据结构，实际是 Long 类型的数组，存储的是文件描述符。当**调用 select()** 时，**内核**根据 IO 状态来修改 fd_set 的内容，由此来通知执行了 select() 的进程哪个 **socket 或 文件** 可读。

  问题：

  - 每次调用 select() 都需要将 fd_set 从**用户态拷贝到内核态**。
  - 同时遍历 fd_set
  - 为了减少拷贝的性能损坏，内核对 fd_set 大小做了限制。

- poll 机制

  poll 机制与 select 类似，只是解决了大小限制问题。

- epoll 机制

  epoll 机制是基于**事件驱动**的IO方式，**解决了大小限制**。

  epoll 提供的函数

  ```c
  int epoll_create(int size);
  int epoll_ctl(...);
  int epoll_wait(...);
  ```

  - epoll_create 创建**epoll 句柄**，函数声明要监听的文件描述符数量
  - epoll_ctl 函数在**内核**中**注册**要监听的文件描述符，只需一次用户态到内核态拷贝，可以**解决 select/poll 每次调用都需要将 fd_set 从用户态拷贝到内核态的问题**。
  - epoll_wait() 函数等待事件的就绪，即**事件驱动**，可以**解决需要遍历 fd_set 的问题**。

![img](/Users/bytedance/GithubProject/basic-knowledge/03. 操作系统/..\img\0f483f2437ce4ecdb180134270a00144~tplv-k3u1fbpfcp-watermark.image)

## 读事件

句柄 socket 从不可读变成可读，或者句柄写缓冲区有新的数据进来。

## 写事件

句柄 socket 从不可写变成可写，或者句柄写缓冲区有新的数据进来。

## 两种模型

### 水平触发 LT

只要句柄 socket 缓冲区一直是可用状态就会**一直触发事件**。只有当读缓冲区读空或者写缓冲区写满后，句柄变为不可用。

LT支持阻塞和非阻塞两种方式，默认的模式就是 LT。

**优缺点**

在LT下应用层的业务逻辑比较简单，更不容易遗漏事件，更不容易出错，但是效率要比ET低。

### 边沿触发 ET

句柄 socket 在发生读写事件时**只会通知用户一次**。ET模式**主要关注**句柄从不可用到可用或者可用到不可用的情况。
ET只支持非阻塞模式。

**应用层逻辑**

ET模式下读写操作要使用wihle循环，直到读/写够足够多的数据，或者读/写到返回EAGAIN。尤其时在写大块数据时，一次write操作不足以写完全部数据，或者在读大块数据时，应用层缓冲区数据太小，一次read操作不足以读完全部数据，应用层要么一直调用while循环一直IO到EGAIN,或者自己调用epoll_ctl手动触发ET响应。

**优缺点**

应用层业务逻辑复杂，容易遗漏事件，很难用好。但是比LT模式效率高。

### 比较

#### ET比LT高效原因

ET在通知用户后，就会把fd从就绪队列里删除。

而LT通知用户后fd还在就绪链表中，随着fd的增多，就绪链表越大。下次epoll要通知用户时还需要遍历整个就绪链表。遍历的性能是线性，如果fd的数量非常多，就会带来比较显著的效率下降。

同样数量的fd下，LT模式维护的就绪链表比ET的大。

#### 非阻塞模式下的accept采用什么触发模式

LT和ET各有优缺点，使用哪种模式取决于并发量。

当并发量比较小时，比较推荐LT，因为LT模式下应用的读写逻辑比较简单，不容易遗漏事件，代码不易出错好维护，而且性能损失不大。

当并发量非常大时，推荐使用ET模式，可以有效提升EPOLL效率。

#### 如何处理LT模式下写事件一直触发

- 方法1：需要向socket写数据的时候才把socket加入epoll，等待可写事件。接受到可写事件后，调用write或者send发送数据。当所有数据都写完后，把socket移出epoll。

  这种方式的缺点是，即使发送很少的数据，也要把socket加入epoll，写完后在移出epoll，有一定操作代价。

- 方法2：开始不把socket加入epoll，需要向socket写数据的时候，直接调用write或者send发送数据。如果返回**EAGAIN**，把socket加入epoll，在epoll的驱动下写数据，全部数据发送完毕后，再移出epoll。

  这种方式的优点是：数据不多的时候可以避免epoll的事件处理，提高效率。

  > EAGAIN 这个错误表示资源暂时不够，能read时，读缓冲区没有数据，或者write时，写缓冲区满了。
  >
  > 遇到这种情况，如果是阻塞socket，read/write就要阻塞掉。而如果是非阻塞socket，read/write立即返回-1， 同时errno设置为EAGAIN。
  >
  > 所以，对于阻塞socket，read/write返回-1代表网络出错了。但对于非阻塞socket，read/write返回-1不一定网络真的出错了。可能是Resource temporarily unavailable。这时你应该再试，直到Resource available。
  >
  > 综上，对于non-blocking的socket，正确的读写操作为:
  > 读：忽略掉errno = EAGAIN的错误，下次继续读
  > 写：忽略掉errno = EAGAIN的错误，下次继续写

### 应用

- ET：Nginx
- LT:  Redis

### EPoll 下的 accept

#### 阻塞模式下 accept 问题

考虑这种情况：TCP连接被客户端夭折，即在服务器调用accept之前，客户端主动发送RST终止连接，导致刚刚建立的连接从就绪队列中移出。

如果套接口被设置成阻塞模式，服务器就会一直阻塞在accept调用上，直到其他某个客户建立一个新的连接为止。但是在此期间，服务器单纯地阻塞在accept调用上，就绪队列中的其他描述符都得不到处理。

解决办法是把监听套接口设置为非阻塞，当客户在服务器调用accept之前中止某个连接时，accept调用可以立即返回-1。

#### ET模式下accept问题

考虑这种情况：多个连接同时到达，服务器的TCP就绪队列瞬间积累多个就绪连接，由于是边缘触发模式，epoll只会通知一次，accept只处理一个连接，导致TCP就绪队列中剩下的连接都得不到处理。

解决办法是用while循环抱住accept调用，处理完TCP就绪队列中的所有连接后再退出循环。如何知道是否处理完就绪队列中的所有连接呢？accept返回-1并且errno设置为EAGAIN就表示所有连接都处理完。

**综合以上两种情况，服务器应该使用非阻塞地accept。**

### **应用场景**

- **大数据处理：**因为大数据的数据量比较多，因此一次可能处理不完，可以使用水平触发，来多次处理数据
- **小数据处理：**小数据调用边缘触发即可，一次处理完就行
- **服务器的监听套接字：**使用水平触发。当有客户端连接时如果这次不处理，可以放到下一次来处理。但是如果使用边缘触发，本次不处理，下次再处理就消失了，从而失去了这个客户端的连接

# 信号驱动IO

在unix系统中，应用程序发起IO请求时，可以给IO请求注册一个信号函数，请求立即返回，操作系统底层则处于等待状态（等待数据就绪），直到数据就绪，然后通过信号通知主调程序，主调程序才去调用系统函数recvfrom()完成IO操作。

信号驱动也是一种非阻塞式的IO模型，比起上面的非阻塞式IO模型，**信号驱动式IO模型不需要轮询检查底层IO数据是否就绪**，而是被动接收信号，然后再调用recvfrom执行IO操作。

比起多路复用IO模型来说，信号驱动IO模型**针对的是一个IO的完成过程**， 而多路复用IO模型针对的是多个IO同时进行时候的场景。 信号驱动式IO模型用下图表示

![图片](/Users/bytedance/GithubProject/basic-knowledge/03. 操作系统/..\img\信号驱动IO)

# 异步IO(AIO)

在此种模式下，将整个IO操作（包括等待数据就绪，复制数据到应用程序工作空间）**全都交给操作系统完成**。数据就绪后操作系统将数据拷贝进应用程序运行空间之后，操作系统再通知应用程序，这个过程中应用程序不需要阻塞。

![图片](/Users/bytedance/GithubProject/basic-knowledge/03. 操作系统/..\img\异步IO)

# 误区

**阻塞、非阻塞、多路IO复用，都是同步IO，异步必定是非阻塞的**，所以不存在异步阻塞和异步非阻塞的说法。真正的异步IO需要CPU的深度参与。换句话说，**只有用户线程在操作IO的时候根本不去考虑IO的执行，全部都交给CPU去完成，而只需要等待一个完成信号的时候，才是真正的异步IO。**所以，**fork子线程去轮询、死循环或者使用select、poll、epoll，都不是异步。**

# 适用场景

- BIO方式适用于**连接数目比较小且固定的架构**，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。
- NIO方式适用于**连接数目多且连接比较短（轻操作）的架构**，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。
- AIO方式适用于**连接数目多且连接比较长（重操作）的架构**，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。

https://mp.weixin.qq.com/s?__biz=MzUyNzgyNzAwNg==&mid=2247483941&idx=1&sn=97628f4d69d8607badf39bfeb7557457&scene=21#wechat_redirect

